### 1、集成学习的概念
结合多个个体学习器完成任务
### 2、个体学习器的概念
具有特定算法可构成集成学习的学习器
### 3、boosting bagging的概念、异同点
https://www.cnblogs.com/liuwu265/p/4690486.html

### 4、理解不同的结合策略(平均法，投票法，学习法)
平均法：所有学习器预测结果的平均值作为最终预测结果
投票法：所有学习器预测结果的众数最为最终预测结果
学习法：利用所有学习器的预测结果训练模型得到最终预测结果

### 5、随机森林的思想
Bagging + 决策树 = 随机森林

### 6、随机森林的推广
https://www.cnblogs.com/pinard/p/6156009.html
### 7、随机森林的优缺点
**优点**
1、可以处理高维数据，不同进行特征选择（特征子集是随机选择）
2、模型的泛化能力较强
3、训练模型时速度快，成并行化方式，即树之间相互独立
4、模型可以处理不平衡数据，平衡误差
5、最终训练结果，可以对特种额排序，选择比较重要的特征
6、随机森林有袋外数据（OOB），因此不需要单独划分交叉验证集
7、对缺失值、异常值不敏感
8、模型训练结果准确度高
**缺点**
当数据噪声比较大时，会产生过拟合现象对有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响

### 8、随机森林在sklearn中的参数解释

n_estimators：基础分类器数量
criterion：划分衡量指标
max_depth：决策树最大深度
min_samples_split：决策树叶结点继续分裂最小样本数量
min_samples_leaf：决策树叶结点最小样本数量
min_weight_fraction_leaf：决策树叶结点最小加权样本数量
max_features：搜索划分时考虑的特征数量
max_leaf_nodes：决策树最大叶结点数量
min_impurity_decrease：决策树叶结点最小衡量指标提升
bootstrap：是否进行有放回取样
oob_score：是否通过未参加训练的样本估计模型效果
n_jobs：控制并行
random_state：随机种子
verbose：控制输出
warm_start：是否使用之前的输出
class_weight：类别权重

### 9、随机森林的应用场景
分类问题、回归问题
