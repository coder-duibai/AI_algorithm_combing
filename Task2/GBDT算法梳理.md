# GBDT算法梳理
参考自：[gbdt算法](https://www.jianshu.com/p/b954476a00d9)
GBDT全称Gradient Boosting Decison Tree，同为Boosting家族的一员，它和Adaboost有很大的不同。Adaboost 是利用前一轮弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去，简单的说是Boosting框架+任意基学习器算法+指数损失函数。GBDT也是迭代，也使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同，简单的说Boosting框架+CART回归树模型+任意损失函数。 
　　GBDT的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。

## 一、前向分布算法
对于AdaBoost，可以将其视为一个将多个弱分类器线性组合后对数据进行预测的算法，该模型可以表示为：
![1](https://s2.ax1x.com/2019/08/10/eLSDaR.png)
b(x;ym)为基函数（即单个弱分类器），gamma_m为基函数的参数（即弱分类器中特征的权重向量），beta_m为基函数的系数（即弱分类器在线性组合时的权重），f(x)就是基函数的线性组合。
        给定训练数据和损失函数L(y,f(x))的条件下，构建最优加法模型f(x)的问题等价于损失函数最小化：
![](https://s2.ax1x.com/2019/08/10/eLSWse.png)
这个公式展现了AdaBoost算法的核心过程。

我们可以利用前向分布算法来求解上一个式子的最优参数。前向分布算法的核心是从前向后，每一步计算一个基函数及其系数，逐步逼近优化目标函数式，就可以简化优化的复杂度。
M-1个基函数的加法模型为：
![](https://s2.ax1x.com/2019/08/10/eLSfqH.png)
M个基函数的加法模型：
![](https://s2.ax1x.com/2019/08/10/eLSIII.png)
由上面两式得：
![](https://s2.ax1x.com/2019/08/10/eLSTit.png)
由这个公式和公式(2)得极小化损失函数:
![](https://s2.ax1x.com/2019/08/10/eLSLQS.png)

## 二、负梯度拟合
Freidman提出了梯度提升算法，算法的核心是利用损失函数的负梯度将当前模型的值作为回归问题提升树算法中的残差的近似值，去拟合一个回归树。
        GBDT的思想就是不断去拟合残差，使残差不断减少。用一个通俗的例子来讲假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。（参考集成学习之Boosting-gbdt）GBDT中每次迭代构造的Cart树都是用前一轮的残差拟合的。
        第t轮第i个样本的损失函数的负梯度表示为：
![](https://s2.ax1x.com/2019/08/10/eLSzon.png)
利用(x_i, r_{ti}) (i=1,2,...,m)我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域R_{tj}, j=1,2,...,J其中J为叶子节点个数。
针对每一个叶子节点里的样本，我们求出使损失函数最小的 输出值c_{tj}:
![](https://s2.ax1x.com/2019/08/10/eLpCWV.png)
## 三、损失函数
![](https://s2.ax1x.com/2019/08/10/eLpkyF.png)
## 四、回归
![](https://s2.ax1x.com/2019/08/10/eLpZw9.png)
## 五、二分类，多分类
![](https://s2.ax1x.com/2019/08/10/eLpeoR.png)
## 六、正则化
![](https://s2.ax1x.com/2019/08/10/eLpQSK.png)
## 七、优缺点
**优点：**

可以灵活处理各种类型的数据，包括连续值和离散值

相对于SVM来说，较少的调参可以达到较好的预测效果

使用健壮的损失函数时，模型鲁棒性非常强，受异常值影响小

**缺点:**
由于弱学习器之间存在依赖关系，难以并行训练数据
## 八、sklearn参数
boosting框架相关参数：

n_estimators：弱学习器最大迭代次数/弱学习器个数
learning_rate：每个弱学习器的权重缩减步长（正则化时的a）
subsample：子采样比例
init：初始化时的弱学习器，默认用训练集样本来做样本集的初始化分类回归预测

loss：损失函数

分类模型：对数似然损失函数deviance，指数损失函数exponential
回归模型：均方差ls，绝对损失lad，Huber损失huber，分位数损失quantile


alpha：只有回归模型有，当使用Huber损失和分位数损失时指定的分位数的值

弱学习器参数：

max_features：划分时考虑的最大特征数
max_depth：决策树最大深度
min_samples_split：内部节点划分所需最小样本数
min_samples_leaf：叶子节点最少样本数
min_weight_fraction_leaf：叶子节点最小样本权重，若小于改值则该节点会和兄弟节点一起被剪枝
max_leaf_nodes：最大叶子节点数
min_impurity_split：节点划分最小不纯度

## 九、应用场景
GBDT的适用面非常广，几乎可以用于所有回归问题（线性/非线性），也可以用于分类问题。